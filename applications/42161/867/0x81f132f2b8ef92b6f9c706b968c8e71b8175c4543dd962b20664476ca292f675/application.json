{
  "id": "187",
  "chainId": 42161,
  "roundId": "867",
  "projectId": "0x81f132f2b8ef92b6f9c706b968c8e71b8175c4543dd962b20664476ca292f675",
  "metadata": {
    "signature": "0x349a6d8dc1b178f58e0e0edb08274064cb9070fb7c894a3ab60b65201fb8e6a367e87bf59da0723c0c19de785ceed8b8ae9c09f18c41758418b89bf3d310b61b1b",
    "application": {
      "round": "0x3E93205B786796Cf7Ea70404E89c7dda3b84D07a",
      "answers": [
        {
          "type": "email",
          "answer": "hd@lexon.org",
          "hidden": true,
          "question": "Email Address",
          "questionId": 0
        },
        {
          "type": "short-answer",
          "answer": "@brainiacfive",
          "hidden": true,
          "question": "Telegram Handle We Can Use To Contact You",
          "questionId": 1
        },
        {
          "type": "short-answer",
          "answer": "https://github.com/lexon-ai/lexon",
          "hidden": false,
          "question": "Project GitHub Repo",
          "questionId": 2
        },
        {
          "type": "link",
          "hidden": false,
          "question": "Link to your Public Group Chat",
          "questionId": 3
        },
        {
          "type": "paragraph",
          "hidden": false,
          "question": "Have you participated in a previous Gitcoin Grants Round? If so, please name the rounds or link the builder profile associated with previous rounds.",
          "questionId": 4
        },
        {
          "type": "paragraph",
          "answer": "### Enabling Markets\n\nThe prospect to easily create unbreakable, safe, and legally enforceable agreements in plain English, makes a wealth of new, personal forms of organization and trade possible for the first time, just like Ebay created a global flee market. The historic breakthroughs of both generative and symbolic AI allow this for the first time right now. \n\n### Building Trust\n\nAs the blockchain space seeks to onboard the next billion users, a focus on transparence and ease-of-use is needed—which this Hybrid AI concept brings to both smart contracts and black box generative AI: lifting the veil, giving them both a friendlier appeal and new momentum.\n\n\nHybrid AI can strengthen the image and awareness of smart contracts beyond crypto.\n\n\n\n## Mainstreaming Smart Contracts\n\nNatural language—the common medium of contracting, business, and law—is really the 'interface' that can make crypto and smart contracts **backwards compatible** to the existing OS of the world.\n\n\nHybrid AI can effect a **reduction of hurdles to everyday use of blockchain tech** beyond payments. Long-term, AI smart contracts can become a household name for DIY contracting for any purpose, private or public, personal, community governance, company administration or politics, investment, or art.\n\n\n\n\n",
          "hidden": false,
          "question": "How does your project contribute to the growth of the OSS ecosystem? Please provide specific examples of how your application enhances or expands the current ecosystem.",
          "questionId": 5
        },
        {
          "type": "paragraph",
          "answer": "**A combined generative and symbolic AI system to create controllably accurate smart contracts in a chat dialog.**\n\nLEXON HYBRID AI is a concept to provide a novel, reliable, chat-based way to write smart contracts for EVM-based blockchains in English. Despite being AI-supported, it will offer precision and transparency.\n\n\nThe result will be a web-based, AI-powered chat app for non-programmers to create accurate EVM-based smart contracts, from a free text that describes the contract in any level of detail. \n\n\nA verification dialog powered by hybrid AI that understands the contract—which means: has an internal representation of its meaning—helps to make sure that the contract shapes up precisely as intended. This is what generative AI alone cannot offer.\n\n\n### Your Daily Contract\n\nThe goal is to make easily conjured smart contracts a ubiquitous part of **daily life for everyone** like, e.g., smart phone messenger apps. Not in the form of one-size-fits-all projects but fully bespoke agreements tailored to the billions of situations of real life, to onboard the next billion users to blockchain generally and EVM-based chains specifically.\n\n\n### Broadening Appeal\n\nDone right, AI-supported smart contracts are an obvious, generic and versatile concept that can speak to a wide spectrum of potential new users. At this point in time, “AI” can motivate people to look at blockchain again and find out what progress has been made. In the course, to find out about EVM-based chains and what its advantages are.\n\n\n### Onboarding the Legal Profession\n\nBecause lawyers & judges can read smart contracts written in plain language, they can finally advance the common way that firms are organized, with EVM-based chains as the rails for a consequential cultural and behavioral shift on the grass-roots level in commerce, finance and government.\n\n\n### Different\n\nBecause of how Lexon advances the capabilities of symbolic AI, there is no other hybrid AI system as proposed here. The symbolic part—Lexon's precise language skill—makes the difference.\n\n\n\n\n",
          "hidden": false,
          "question": "What features or use cases does your dApp/App present? How is it differentiated?",
          "questionId": 6
        },
        {
          "type": "paragraph",
          "answer": "The grant funds will be used to advance on this roadmaps to build the platform:\n\n# ROADMAP\n\n### Milestone 01\n\n## I • FUNCTIONAL PROOF OF CONCEPT\n\n\nFull-cycle technical proof that feeds a selection from Lexon grammar, approx. 1,000 pages documentation and 5,000 examples to Google o3 via its context window, finding the optimal learning set to allow o3 to create the best Lexon text.\n\n\nThe setup follows Google’s instructions for providing long context in the fastest and most economical way to achieve the lowest cost, highest hit result during text creation.\n\n\nThe terminal-based proof of concept system feeds the results to the Lexon compiler to create Solidity text.\n\n\nResult:\n\n-\tTerminal-operated essential code base to create the core service.\n\n.\n\n### Milestone 02\n\n## II • OPTIMIZATION AND ALTERNATE MODELS\n\n\nThe LLM results are optimized by tuning the long context inputs and exploration of different examples and styles to write prompts.\n\n\nOther LLMs—online and off, commercial and free—than o3 are revisited and tested to identify possible better suited candidates, measured by accuracy of results, response time, complexity of setup and price.\n\nResult:\n\n-\tSetting realistic expectations of achievable outcomes after optimizing results\n-\tEstablishing expectable costs per use\n-\tSettling for one LLM offering to focus the rest of development on\n\n.\n\n### Milestone 03\n\n## III • GUI AND SERVER CREATION\n\n\nA chat-like web interface is created that interacts with the server that processes user input.\n\n \nThe user enters a contract description prompt in free form, obtains a first response in Lexon’s plain-text code that can be verified for accuracy and redone from scratch or fine-tuned, before submitting to the Lexon compiler to create Solidity from it.\n\n\nThe result is deployed to an EVM-based chain using a connected browser wallet.\n\n\nThe intermediate step of verification is the central contribution of the hybrid setup, which the GUI is to dress with the best possible UX.\n\n\nResult: \n\n-\tFull GUI-controlled roundtrip of contract creation from scratch.\n\n.\n\n### Milestone 04\n\n## IV • USER EXPERIENCE\n\n\nThe user experience is tested by volunteers to tweak the flow and visual presentation.\n\n\nDocumentation, examples, and a tutorial are tied into the app to guide first-time users and provide reference for advanced users.\n\n\n(Note again that no documentation or learning is necessary to read Lexon code.)\n\n\nWeb pages are created, including a landing screen, to host the app and provide the docs.\n\n\nResult: \n\n-\tLaunch-ready app\n-\tDocumentation\n-\tCompanion Web pages\n\n.\n\n### Milestone 05\n\n## V • LAUNCH\n\n\nLive tests, opening to public; source clean up, commenting; repo clean up; github push; final grant reporting.\n\n\nResult: \n\n-\tLive app, website, and documentation\n-\tpublished sources\n-\tproject grant report\n",
          "hidden": false,
          "question": "What are your plans for further development, and how will the grant funds be used to achieve these goals?",
          "questionId": 7
        },
        {
          "type": "paragraph",
          "answer": "n/a",
          "hidden": false,
          "question": "For any project deploying smart contracts on blockchain networks, please list all your deployer addresses and their corresponding blockchain networks. Use this format for each entry: [deployer_address], [chain_id]—for example, 0x123abc..., 42161 (This represents a deployer address on the Arbitrum network). Please include a separate line for each unique deployer address and blockchain combination.",
          "questionId": 8
        }
      ],
      "project": {
        "id": "0x81f132f2b8ef92b6f9c706b968c8e71b8175c4543dd962b20664476ca292f675",
        "title": "Hybrid AI",
        "logoImg": "bafkreicrbynk75jdla6i2vnnma3q3cfnljpuxrzyzetc6ufua5umlki3l4",
        "metaPtr": {
          "pointer": "bafkreibba3osdo5r6ky2i5vlgwfflacccr5gpssyo6rt5t4wsslx6xyjdm",
          "protocol": "undefined"
        },
        "website": "https://lexon.org/ai.html",
        "bannerImg": "bafkreidpjgp546p2glvbt4mpcu3zkmvm75z6bbo7bdh624tt6t3yn7ewf4",
        "createdAt": 1732106722217,
        "userGithub": "lexonian",
        "credentials": {
          "github": {
            "type": [
              "VerifiableCredential"
            ],
            "proof": {
              "type": "EthereumEip712Signature2021",
              "created": "2025-03-31T16:04:15.150Z",
              "@context": "https://w3id.org/security/suites/eip712sig-2021/v1",
              "proofValue": "0x66fae113ea16cf4e11aeffbdd35479d5f600a80856917a8f703c4e92280c6bc73dec46c2fcd717af6e277841c486efa3d13e46f58a297679aed12b1896ffe7be1c",
              "eip712Domain": {
                "types": {
                  "Proof": [
                    {
                      "name": "@context",
                      "type": "string"
                    },
                    {
                      "name": "created",
                      "type": "string"
                    },
                    {
                      "name": "proofPurpose",
                      "type": "string"
                    },
                    {
                      "name": "type",
                      "type": "string"
                    },
                    {
                      "name": "verificationMethod",
                      "type": "string"
                    }
                  ],
                  "@context": [
                    {
                      "name": "nullifiers",
                      "type": "NullifiersContext"
                    },
                    {
                      "name": "provider",
                      "type": "string"
                    }
                  ],
                  "Document": [
                    {
                      "name": "@context",
                      "type": "string[]"
                    },
                    {
                      "name": "credentialSubject",
                      "type": "CredentialSubject"
                    },
                    {
                      "name": "expirationDate",
                      "type": "string"
                    },
                    {
                      "name": "issuanceDate",
                      "type": "string"
                    },
                    {
                      "name": "issuer",
                      "type": "string"
                    },
                    {
                      "name": "proof",
                      "type": "Proof"
                    },
                    {
                      "name": "type",
                      "type": "string[]"
                    }
                  ],
                  "EIP712Domain": [
                    {
                      "name": "name",
                      "type": "string"
                    }
                  ],
                  "CredentialSubject": [
                    {
                      "name": "@context",
                      "type": "@context"
                    },
                    {
                      "name": "nullifiers",
                      "type": "string[]"
                    },
                    {
                      "name": "id",
                      "type": "string"
                    },
                    {
                      "name": "provider",
                      "type": "string"
                    }
                  ],
                  "NullifiersContext": [
                    {
                      "name": "@container",
                      "type": "string"
                    },
                    {
                      "name": "@type",
                      "type": "string"
                    }
                  ]
                },
                "domain": {
                  "name": "VerifiableCredential"
                },
                "primaryType": "Document"
              },
              "proofPurpose": "assertionMethod",
              "verificationMethod": "did:ethr:0xd6f8d6ca86aa01e551a311d670a0d1bd8577e5fb#controller"
            },
            "issuer": "did:ethr:0xd6f8d6ca86aa01e551a311d670a0d1bd8577e5fb",
            "@context": [
              "https://www.w3.org/2018/credentials/v1",
              "https://w3id.org/vc/status-list/2021/v1"
            ],
            "issuanceDate": "2025-03-31T16:04:15.149Z",
            "expirationDate": "2025-06-29T16:04:15.149Z",
            "credentialSubject": {
              "id": "did:pkh:eip155:1:0x05f6efaa0E7F00dB09C1f7fd91F7e7f5ec92e19c",
              "@context": {
                "provider": "https://schema.org/Text",
                "nullifiers": {
                  "@type": "https://schema.org/Text",
                  "@container": "@list"
                }
              },
              "provider": "ClearTextGithubOrg#lexon-ai#193349241",
              "nullifiers": [
                "v0.0.0:JUVl30LDg+JDqnuXA3bOSlq1v7Fz0mwVAjaLazi1ZNU="
              ]
            }
          },
          "twitter": {
            "type": [
              "VerifiableCredential"
            ],
            "proof": {
              "type": "EthereumEip712Signature2021",
              "created": "2025-03-31T15:32:04.550Z",
              "@context": "https://w3id.org/security/suites/eip712sig-2021/v1",
              "proofValue": "0x35be160c181eced34f86cfee08eaeecbb984c2855039479d67ddfd6c9a10a9ac2252bf2c6e6621fb0661414f6ec055386087a422e25218b4211e088f5dbd7de91c",
              "eip712Domain": {
                "types": {
                  "Proof": [
                    {
                      "name": "@context",
                      "type": "string"
                    },
                    {
                      "name": "created",
                      "type": "string"
                    },
                    {
                      "name": "proofPurpose",
                      "type": "string"
                    },
                    {
                      "name": "type",
                      "type": "string"
                    },
                    {
                      "name": "verificationMethod",
                      "type": "string"
                    }
                  ],
                  "@context": [
                    {
                      "name": "nullifiers",
                      "type": "NullifiersContext"
                    },
                    {
                      "name": "provider",
                      "type": "string"
                    }
                  ],
                  "Document": [
                    {
                      "name": "@context",
                      "type": "string[]"
                    },
                    {
                      "name": "credentialSubject",
                      "type": "CredentialSubject"
                    },
                    {
                      "name": "expirationDate",
                      "type": "string"
                    },
                    {
                      "name": "issuanceDate",
                      "type": "string"
                    },
                    {
                      "name": "issuer",
                      "type": "string"
                    },
                    {
                      "name": "proof",
                      "type": "Proof"
                    },
                    {
                      "name": "type",
                      "type": "string[]"
                    }
                  ],
                  "EIP712Domain": [
                    {
                      "name": "name",
                      "type": "string"
                    }
                  ],
                  "CredentialSubject": [
                    {
                      "name": "@context",
                      "type": "@context"
                    },
                    {
                      "name": "nullifiers",
                      "type": "string[]"
                    },
                    {
                      "name": "id",
                      "type": "string"
                    },
                    {
                      "name": "provider",
                      "type": "string"
                    }
                  ],
                  "NullifiersContext": [
                    {
                      "name": "@container",
                      "type": "string"
                    },
                    {
                      "name": "@type",
                      "type": "string"
                    }
                  ]
                },
                "domain": {
                  "name": "VerifiableCredential"
                },
                "primaryType": "Document"
              },
              "proofPurpose": "assertionMethod",
              "verificationMethod": "did:ethr:0xd6f8d6ca86aa01e551a311d670a0d1bd8577e5fb#controller"
            },
            "issuer": "did:ethr:0xd6f8d6ca86aa01e551a311d670a0d1bd8577e5fb",
            "@context": [
              "https://www.w3.org/2018/credentials/v1",
              "https://w3id.org/vc/status-list/2021/v1"
            ],
            "issuanceDate": "2025-03-31T15:32:04.549Z",
            "expirationDate": "2025-06-29T15:32:04.549Z",
            "credentialSubject": {
              "id": "did:pkh:eip155:1:0x05f6efaa0E7F00dB09C1f7fd91F7e7f5ec92e19c",
              "@context": {
                "provider": "https://schema.org/Text",
                "nullifiers": {
                  "@type": "https://schema.org/Text",
                  "@container": "@list"
                }
              },
              "provider": "ClearTextTwitter#hdiedrich",
              "nullifiers": [
                "v0.0.0:nf4DJjQwCmNm5eETjF/IvAJDU2pWgKb1951MaHy/P0Y="
              ]
            }
          }
        },
        "description": "\n# Hybrid AI for Reliable Smart Contract Creation.\n\n\n**A combined generative and symbolic AI system to create controllably accurate smart contracts in a chat dialog.**\n\nLEXON HYBRID AI is a concept to provide a novel, reliable, chat-based way to write smart contracts for EVM-based blockchains in English. Despite being AI-supported, it will offer precision and transparency.\n\n\nThe solution is a unique AI system that combines an LLM with a symbolic AI toolchain in a new way, powered by deterministic program code that reads like plain English: the [Lexon](https://www.lexon.org) compiler and grammar.  (See this [grant](https://explorer.gitcoin.co/#/projects/0x5ab014eb31f6c66475d6f20be8a1855eea5d95b7699e1012c3d3dc4a92a58954)).\n\n\nWhile the LLM service plays an essential role in this architecture, the center of gravity lies with the symbolic AI component that is based on the [Lexon language](https://www.lexon.org/vocabulary.html), which provides full transparency, is completely accessible, and easily malleable to best support the user in a chat dialog. Different from prompt-based engineering, achieving the **intended** outcome is significantly simplified for the user.\n\n\nThe proposed approach is the result of experimentation with Google oX, Microsoft co-pilot, and the Lexon syntax and compiler, to arrive at the most effective way to join generative and symbolic AI for the creation of robust code. \n\n\nWhere an LLM provides unrestricted freedom of expression, the symbolic AI provides the uncompromising reliability that is required to create a smart contract.\n\n.\n\n## The App\n\n\nThe result will be a web-based, AI-powered chat app for non-programmers to create accurate EVM-based smart contracts, from a free text that describes the contract in any level of detail. \n\n\nA verification dialog powered by hybrid AI that understands the contract—which means: has an internal representation of its meaning—helps to make sure that the contract shapes up precisely as intended. This is what generative AI alone cannot offer.\n\n\n\n\nThis addresses the problem that non-programmers—businesspeople, legal professionals, or private citizens—know better what they want to express than the programmers they have to work with but today cannot write nor verify smart contracts themselves. This app makes everything plain English.\n\n\nBecause with the absolute accuracy that smart contracts require, the occasional slip-ups of LLMs are problematic. The symbolic AI component of this project adds full accuracy and more productive fine-tuning.\n\n\nThe relative opacity of crypto programming as well as gen AI has hampered the mainstreaming of blockchain tech. The verification dialog flow described below provides transparency with every step.\n\n.\n\n## Flow\n\nA user will\n\n-\tenter a free description of a smart contract to be created in a chat box\n\n-\texamine the resulting proposed algorithm in English\n\n-\trework the prompt, **or the plain-text algorithm** (native-text program)\n\n-\tto finally deploy the smart contract to EVM-based chains.\n\n\n\nBehind the scenes,\n\n\n**(#1 / generative and free-form:)** the first stage is a communication with an LLM that has the capability to learn domain specific knowledge (“long context”), and is trained to create Lexon texts, i.e., write texts in the special programming language for smart contracts that reads like English and can be compiled to Solidity (see below).\n\n\n**(#2 / symbolic and exact:)** The LLM output is checked by the user, then compiled to Solidity by the Lexon compiler. The end result is Solidity code that precisely follows the English “pseudo code”—like algorithm presented in the Lexon text. It has no hallucinations because it is translated from English by fully deterministic compiler technology, rather than machine learning. This is a more restricted procedure that is better suited to achieving accuracy and full transparency.\n\n.\n\n## Mainstreaming Smart Contracts\n\n\nEVM-based chains and freely dictated smart contracts are a perfect fit, because Lexon “speaks Solidity” to effect a **reduction of hurdles to everyday use of blockchain tech** beyond payments. Long-term, together, AI smart contracts on EVM-based chains can become the household name for DIY contracting for any purpose, private or public, personal, or community governance, company administration or politics, investment, or art.\n\n\nThe goal is to make easily conjured smart contracts a ubiquitous part of **daily life for everyone** like, e.g., smart phone messenger apps. Not in the form of one-size-fits-all projects but fully bespoke agreements tailored to the billions of situations of real life, to onboard the next billion users to blockchain generally and EVM-based chains specifically.\n\n.\n\n## Why Now?\n\nGen AI has fired up the imagination of millions about new, personalized ways to digitally interact. This creates the opportunity to mainstream smart contract use with the help of AI, as a ubiquitous, personal technology, just like everyone is using chats or Excel today. Its simplicity, combined with its novel twist, makes the proposed Hybrid AI concept a contender for the blockchain-AI killer app.\n\nThe prospect to easily create unbreakable, safe, and legally enforceable agreements in plain English, makes a wealth of new, personal forms of organization and trade possible for the first time, just like Ebay created a global flee market. The historic breakthroughs of both generative and symbolic AI allow this for the first time right now. \n\nAs the blockchain space seeks to onboard the next billion users, a focus on transparence and ease-of-use is needed—which Lexon brings to both smart contracts and black box generative AI: lifting the veil, giving them both a friendlier appeal and new momentum.\n\n.\n\n## How will Lexon AI grow the Onchain Economy?\n\nLexon AI can strengthen the image and awareness of smart contracts beyond crypto.\n\n\nDone right, AI-supported smart contracts are an obvious, generic and versatile concept that can speak to a wide spectrum of potential new users. At this point in time, “AI” can motivate people to look at blockchain again and find out what progress has been made. In the course, to find out about EVM-based chains and what its advantages are.\n\n\nBecause lawyers & judges can read smart contracts written in plain language, they can finally advance the common way that firms are organized, with EVM-based chains as the rails for a consequential cultural and behavioral shift on the grass-roots level in commerce, finance and government.\n\n\nNatural language—the common medium of contracting, business, and law—is really the 'interface' that can make crypto and smart contracts **backwards compatible** to the existing OS of the world.\n\n.\n\n## Who is the Target Audience?\n\n**Everyone** — the approach is designed to broaden smart contract and DAO adoption by including non-programmers and non-tech-affine users. This is a 10kx augmentation of the potential user pool.\n\n\nLike Apple pushed for the idea of a computer in everyone's living room, smart contracts should likewise become an everyday normal phenomenon. This goal needs the no-code transparency of Lexon, to allow for smart contracts and DAOs in every walk of life, so smart contracts can become a part of school education and the definition of literacy, the standard way to manage any community or private undertaking.\n\n\nLexon AI targets everyday citizens, philosophers, politicians, businesspeople—who are experts in contracts to the same degree that lawyers are—as well as finance specialists, investors, DAO communities, or artists.\n\n\nAs a matter of fact, lawyers have been excited about writing Lexon smart contracts. Were used as legal agreements, judges will be a relevant target group as readers.\n\n.\n.\n\n\n# DETAILS\n\n\n## Rational\n\n\nThis HYBRID AI approach offers a generalizable solution to the mishaps (or occasional voluntary incompleteness) that code creation with LLMs can suffer, even with its reasoning models.\n\n\nIt will be be the more valuable the more difficult it will turn out eventually for an LLM to completely overcome hallucinations. Because LLM output is a creative aggregation of its learning material, it appears possible that a hard limit will remain especially regarding programs that are genuinely new. This Hybrid AI approach can be the cure.\n\n\nOn top of that, LLMs can give up on code for a modest level of nested requirements. This is still frequently the case for real tasks, and it remains unclear when, if ever, gen AI will rise to the level of abstraction needed for the creation of complete programs.\n\n\nThe sentiment has recently shifted to looking at LLMs as a productivity tool rather than a full replacement of programmers (supported by experiences in different industries: https://www.nytimes.com/2025/03/29/opinion/ai-tech-innovation.html). Other sources report that AI mostly helps junior developers; with only small gains reported for more difficult assignments, and for senior dev users (this including the well-known report that is used to advertise Microsoft’s co-pilot. The reported gains are clearly leaning towards novice users).\n\n\nThis hybrid AI concept, however, DOES intend to replace programmers. Therefore, a different architecture is proposed.\n\n\nThe described hybrid combination (above) of generative AI with symbolic AI, has been practically determined as the most robust approach. The joining of machine learning with symbolic elements like reasoning is in full swing across the AI industry, after the limits of reinforced learning for gen AI have been reached. This comprises a fusion of concepts similar to the one proposed here. But the way in which components are composed here is new:\n\n.\n\n## Approach\n\n\nTests with different approaches to combine symbolic and generative AI resulted in the specific method presented here. A possibly more intuitive attempt (namely, to have generative AI write Solidity programs based on a non-ambiguous Lexon text) failed and is less attractive for an additional reason. Tests used real U.S. law (Intestate Estates, U.C.C. financing) and well-researched, realistic Lexon text examples to test different combinations of generative and symbolic AI engines under near-real world conditions.\n\n\nThe mentioned naive sequence of **(a)** writing a code specification in the Lexon syntax, i.e., in English, for the LLM to create Solidity code, did not succeed, and the emerging class of problems looks difficult to overcome. In general, the LLM tended to overlook central keywords, even though they are in plain English and that in a Lexon text carry fundamental logical significance, (like object definitions, or loops); or the LLM declined to code the complete contract when requirements got more complex, proposing that the user contribute the missing (deeper nested) parts (e.g., for the U.S. law for intestate estates).\n\n\nIn the tests, the utilization proposed here furnished the best results, i.e.: **(b)** letting the LLM write Lexon texts and compile them using the Lexon compiler. As a most valuable and welcome advantage, with this flow, hallucinations of the LLM are caught and eliminated by the human writer in the intermediary dialog steps that also allow a non-programmer to verify the created contract logic in English, before it is 100% deterministically translated to Solidity (as opposed to the naïve flow **(a)** where the LLM creates Solidity code).\n\n\nThis makes the proposed constellation **(b)** of machine-learning and compiler technology the path of choice.\n\n.\n\n## Outlook\n\n\n* In future editions, the proposed app will receive a **voice dictation** front end to allow for smart contract creation without typing.\n\n\n* An iteration of the system will offer **GUI generation** for the created smart contracts, which can be comprehensive because the plain-text code of Lexon supports the generation of an interface better than normal 3rd generation language code, as it is information-richer as the starting point of a GUI, thanks to its linguistic nature.\n\n\n.\n.\n\n# THE CREATION OF HYBRID AID\n\n## Linguistic AI \n\nThe intermediary language that drives the app’s chat, Lexon, **defines a subset of English as a programming language** so that natural language texts themselves become the program. \n\n\nThis is the way to arrive at **precise execution.**\n\n\n That symbolic AI should be the solution, is unsurprising, given that programming is the home-game of symbolic AI, as opposed to the more data-centric generative AI. The challenge of our time is to identify the best way of interconnecting the two, unleashing linguistic expressiveness without losing exactness. The proposed architecture breaks new ground, based on Lexon’s unique contribution on the symbolic side.\n\n\n\n**Lexon smart contracts read like normal English.** They are accessible, readable for anyone, as they come. The Lexon compiler translates such plain-text smart contracts to Solidity, with full accuracy and determinism. There are no hallucinations, as Lexon is NOT based on gen AI. It is a linguistic feat instead: “simply” a programming language that is readable like English.\n\n\n\nThe technology fundamentally works like any compiler, the twist is how natural language is accepted in its richness. The main technical achievement of Lexon is the definition and implementation of a **non-ambiguous** subset of the English grammar. It turns out that this is the catalyst for a quantum leap.\n\n\n.\n\n## Historic Development\n\nThis is the result of a long scientific quest. The vision of “normal text as a program” is a centuries-old philosophical project that is today interpreted as the beginning of computer sciences in the 17th century and the foundation of modern mathematics in the 19th. The biggest names in the history of logic are associated with it, the result was today’s computer technology as well as AI.\n\n\nThis research was not originally associated with artificial intelligence but was pursued for the logical clarity of expression that it was hoped it would bring. However, because this path of exploration led to the development of computers, arguably, computers were created for this.\n\n\nAs such, symbolic AI, and “universal language” research were what started computer science, and paved the way to computers. To achieve “programming with language,” beginning with Descartes and Leibniz, through the game-changing contributions of Frege, was the north star of the journey from ancient Greek logic to modern computers. (See Characteristica Universalis https://www.amazon.com/dp/B0D8Y6F8DW).\n\n\nThe combination of the current apex of this technology with generative AI is potentially a crowning achievement, enabling a quantum leap in productivity, constituting a milestone in the history of programming. This high ambition may appear more plausible in light of the fact that the proposed concept is NOT hoping to replace all human intervention in the act of programming—but “only” the expertise of programmers. This goal is in keeping with the historically demonstrated strengths of symbolic AI, which led to the creation of expert systems.\n\n\nThe current roadmap for the Lexon language has its focus on extending the Lexon grammar for **DAOs/** This grant’s trajectory will directly combine with the coming advanced DAO syntax, which is the next milestone.\n\n.\n\n## Combining Generative and Symbolic AI\n\n\nThe main differentiator to a generative AI approach of programming by prompt is that with Lexon, **language itself is interpreted like a program,** not merely used as the description of a program. The difference is fundamental, both on the technical and conceptual level. The result is a decisively more transparent, reliable and safe workflow.\n\n\nThe approach is good for different objectives: for DIY smart contracts that are short, as well as for complex contracts, like DAOs. Lexon is portable to 60 hardware platforms, works off-line, is very fast, and fiercely resource-optimized—many magnitudes more resource-friendly than gen AI.\n\n\n.\n\n## The Hybrid AI Concept\n\n\nThere is a growing recognition that pure, generative AI might disappoint the expectations that have been building.\n\n\nIndustry-wide, a consensus is forming that hybrid AI is the way forward for AI. That would be the combination of generative and symbolic AI in one of many possible ways. And under the hood, all big AI services already implement symbolic AI elements to augment the power of their generative AI offerings.\n\n\nBut unbelievably, on the research side, an unnecessary blood feud between the research communities—generative and symbolic—is holding up progress. The animosity between researchers of the different AI camps is going for decades, is personal, and bears an unscientific take-no-prisoners mentality. There has been open advocacy for defunding the respective other branch (cf. Nobel-laureate Hinton’s advice to the EU to not fund symbolic AI).\n\n\nAccordingly, some very smart people find it plausible that Lexon could be a milestone in the history of logic, while others dismiss its approach wholesale.\n\n\nFundamentally, while gen AI and symbolic AI are similar (see tensors, below), Lexon has a different workflow and different strengths than LLMs, making it good at things that are perfectly complementary to gen AI. \n\n\nIt makes the blueprint of machine execution transparent to humans; it does NOT propose to think or write for the user but “only” to help with the programming and clarification of thoughts (by means of a restricted grammar); it can visualize its understanding (cf. --tree option). This is on each count the opposite of gen AI.\n\n\nNotably, Lexon has an **internal representation of meaning** of a text that generative AI does not have. This is what allows for precise processing of the exact meaning of a text. How it is parsed and understood by the Lexon AI is the topic of the books and papers on Lexon (below). It is a new approach that reaches beyond the meaning of individual words to the network of relationships they form.\n\n\nLexon thus provides unfailing accessibility, and perfect agency: a text is just exactly what it is. It is **aligned and transparent by design.**\n\n\nHowever, for a convincing impression of how **similar** symbolic AI Lexon code looks to generative AI tensors consider the optics of this Lexon parser module: https://github.com/lexonian/lexon/blob/dbb46e554c7c444bbf3d114c15a1f6faa11f76ff/build/scanner.c#L456  \n\n\nThese Lexon tensor tables are the machine-translated model of the Lexon grammar (https://github.com/lexonian/lexon/blob/master/grammar/english.lgf). The difference is that they are magnitudes smaller than gen AI tensors. This is, **because they are precise.** Lexon’s way is not to learn from billions of examples but to incorporate one precise grammar description and operate on that (https://github.com/lexonian/lexon/blob/master/grammar/english.lgf). This is reflected in these matrices that are the core element for text analysis: they are small and exact. Basically, this is the difference between symbolic AI and generative AI, marking their different strengths and weaknesses.\n\n\n### Documentation on Lexon\n\n\nThe following is information on Lexon, which is the symbolic half of the AI system.\n\n\nPlease note that to READ Lexon texts, NO preparation is necessary.\n\n\nThe Lexon subset of English is optimized for readability—because any program, and contract, is more often read than written (*Brian J. Fox*).\n\n\nMost documentation below revolves around *writing* Lexon—which this grant application strives to greatly simplify. Some literature on Lexon is scientific and philosophic, comparing it to similar approaches and suggesting its place in the history of logic.\n\n\nLexon is documented in articles, books, and academic papers, and supported by hands-on documentation: a manual, tutorial, etc. The central place to learn about it is the lexon site at github.com/lexonian/lexon.\n\n\nThe source code is at github.com/lexonian/lexon.\n\n* Writer’s Manual\t\thttps://lexon.org/docs/Lexon%20Manual%200.3.pdf\n* Writer’s Tutorial \t\thttps://lexon.org/docs/Lexon%20Tutorial%200.3.pdf\n* Vocabulary \t\t\thttps://lexon.org/vocabulary.html\n* Examples \t\t\thttps://lexon.org/examples.html\n* Resources \t\t\thttps://lexon.org/resources.html\n* Papers \t\t\thttps://lexon.org/papers.html\n* Books \t\t\thttps://lexon.org/books.html\n* Site \t\t\t\thttps://lexon.org\n\n\n\n.\n.\n\n# ROADMAP\n\n### Milestone 01\n\n## I • FUNCTIONAL PROOF OF CONCEPT\n\n\nFull-cycle technical proof that feeds a selection from Lexon grammar, approx. 1,000 pages documentation and 5,000 examples to Google o3 via its context window, finding the optimal learning set to allow o3 to create the best Lexon text.\n\n\nThe setup follows Google’s instructions for providing long context in the fastest and most economical way to achieve the lowest cost, highest hit result during text creation.\n\n\nThe terminal-based proof of concept system feeds the results to the Lexon compiler to create Solidity text.\n\n\nResult:\n\n-\tTerminal-operated essential code base to create the core service.\n\n.\n\n### Milestone 02\n\n## II • OPTIMIZATION AND ALTERNATE MODELS\n\n\nThe LLM results are optimized by tuning the long context inputs and exploration of different examples and styles to write prompts.\n\n\nOther LLMs—online and off, commercial and free—than o3 are revisited and tested to identify possible better suited candidates, measured by accuracy of results, response time, complexity of setup and price.\n\nResult:\n\n-\tSetting realistic expectations of achievable outcomes after optimizing results\n-\tEstablishing expectable costs per use\n-\tSettling for one LLM offering to focus the rest of development on\n\n.\n\n### Milestone 03\n\n## III • GUI AND SERVER CREATION\n\n\nA chat-like web interface is created that interacts with the server that processes user input.\n\n \nThe user enters a contract description prompt in free form, obtains a first response in Lexon’s plain-text code that can be verified for accuracy and redone from scratch or fine-tuned, before submitting to the Lexon compiler to create Solidity from it.\n\n\nThe result is deployed to an EVM-based chain using a connected browser wallet.\n\n\nThe intermediate step of verification is the central contribution of the hybrid setup, which the GUI is to dress with the best possible UX.\n\n\nResult: \n\n-\tFull GUI-controlled roundtrip of contract creation from scratch.\n\n.\n\n### Milestone 04\n\n## IV • USER EXPERIENCE\n\n\nThe user experience is tested by volunteers to tweak the flow and visual presentation.\n\n\nDocumentation, examples, and a tutorial are tied into the app to guide first-time users and provide reference for advanced users.\n\n\n(Note again that no documentation or learning is necessary to read Lexon code.)\n\n\nWeb pages are created, including a landing screen, to host the app and provide the docs.\n\n\nResult: \n\n-\tLaunch-ready app\n-\tDocumentation\n-\tCompanion Web pages\n\n.\n\n### Milestone 05\n\n## V • LAUNCH\n\n\nLive tests, opening to public; source clean up, commenting; repo clean up; github push; final grant reporting.\n\n\nResult: \n\n-\tLive app, website, and documentation\n-\tpublished sources\n-\tproject grant report\n\n\n.\n.\n\n# TEAM\n\n\n**Henning Diedrich** (development) created Lexon, wrote the first book on Ethereum, was the first technical architect for IBM's blockchain, first director for blockchain of BCG, consulted for the European Commission, the Singapore SWF Temasek, LUMA and Art Basel. He architected the Diamond Blockchain TRACR for DeBeers.\n\n\n**Marco Heinrich** (test contract creation, community support) assists development taking care of prototyping, content creation, and testing. He has previously worked on commercial programming assignments with Henning as trusted and diligent supporter, and participated in Lexon from the start.\n\n\n**Daniel Nemet** (test creation, community reachout) is a community builder in the blockchain space with a special interest in user experience and making blockchain accessible. He worked as consultant in the renewables and eco-space. Currently builds the NYM community and writes a Ph.D. on crypto.\n\n\n\n### Telegram\n\n\nhttps://t.me/lexonians",
        "lastUpdated": 0,
        "projectGithub": "lexon-ai",
        "projectTwitter": "hdiedrich"
      },
      "recipient": "0x855f553a5BA0Defa37f7e8d41948B34a237cB6F4"
    }
  },
  "status": "APPROVED",
  "project": {
    "metadata": {
      "type": "project",
      "canonical": {
        "chainId": 1,
        "registryAddress": "0x4AAcca72145e1dF2aeC137E1f3C5E3D75DB8b5f3"
      }
    }
  }
}